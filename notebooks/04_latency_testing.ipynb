{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AWS Insurance Demo - Latency Testing\n",
        "\n",
        "This notebook demonstrates how to benchmark the Feast feature server for insurance use cases.\n",
        "\n",
        "## Use Case Latency Requirements\n",
        "\n",
        "| Use Case | Transaction Type | Target P99 |\n",
        "|----------|-----------------|------------|\n",
        "| Auto Underwriting | Real-Time (PCM) | < 50ms |\n",
        "| Quick Quote | Real-Time (PCM) | < 20ms |\n",
        "| Claims Assessment | Batch | < 200ms |\n",
        "| Fraud Detection | Streaming (DSS) | < 30ms |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import time\n",
        "import statistics\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Feature server URL (update if running on different host/port)\n",
        "FEATURE_SERVER_URL = \"http://localhost:6566\"\n",
        "\n",
        "print(f\"Feature Server URL: {FEATURE_SERVER_URL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Latency Test\n",
        "\n",
        "Simple synchronous test to verify the feature server is responding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def quick_latency_test(feature_service: str, entity_key: str, entity_values: list, num_requests: int = 10):\n",
        "    \"\"\"Quick synchronous latency test.\"\"\"\n",
        "    url = f\"{FEATURE_SERVER_URL}/get-online-features\"\n",
        "    payload = {\n",
        "        \"feature_service\": feature_service,\n",
        "        \"entities\": {entity_key: entity_values},\n",
        "        \"full_feature_names\": False,\n",
        "    }\n",
        "    \n",
        "    latencies = []\n",
        "    errors = 0\n",
        "    \n",
        "    for i in range(num_requests):\n",
        "        start = time.perf_counter()\n",
        "        try:\n",
        "            response = requests.post(url, json=payload, timeout=30)\n",
        "            latency_ms = (time.perf_counter() - start) * 1000\n",
        "            if response.status_code == 200:\n",
        "                latencies.append(latency_ms)\n",
        "            else:\n",
        "                errors += 1\n",
        "                if i == 0:\n",
        "                    print(f\"Error: {response.text[:200]}\")\n",
        "        except Exception as e:\n",
        "            errors += 1\n",
        "            if i == 0:\n",
        "                print(f\"Error: {e}\")\n",
        "    \n",
        "    if latencies:\n",
        "        sorted_l = sorted(latencies)\n",
        "        print(f\"Feature Service: {feature_service}\")\n",
        "        print(f\"Batch Size: {len(entity_values)}\")\n",
        "        print(f\"Requests: {num_requests} ({errors} errors)\")\n",
        "        print(f\"Latency (ms): mean={statistics.mean(latencies):.2f}, \"\n",
        "              f\"p50={sorted_l[len(sorted_l)//2]:.2f}, \"\n",
        "              f\"p99={sorted_l[int(len(sorted_l)*0.99)]:.2f}\")\n",
        "        return latencies\n",
        "    else:\n",
        "        print(f\"All {num_requests} requests failed!\")\n",
        "        return []\n",
        "\n",
        "# Test with a simple feature service\n",
        "print(\"Quick Latency Test\")\n",
        "print(\"=\" * 60)\n",
        "latencies = quick_latency_test(\n",
        "    feature_service=\"benchmark_small\",\n",
        "    entity_key=\"customer_id\",\n",
        "    entity_values=[\"CUST00000001\"],\n",
        "    num_requests=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comprehensive Benchmark\n",
        "\n",
        "Run the full benchmark suite using the benchmark script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the benchmark script with the standard suite\n",
        "!cd ../scripts && python benchmark_online_server.py \\\n",
        "    --server-url {FEATURE_SERVER_URL} \\\n",
        "    --suite standard \\\n",
        "    --batch-sizes 1,10,50 \\\n",
        "    --num-requests 50 \\\n",
        "    --concurrency 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch Size Impact Analysis\n",
        "\n",
        "Test how latency scales with batch size for the underwriting use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_customer_ids(n):\n",
        "    return [f\"CUST{random.randint(1, 10000):08d}\" for _ in range(n)]\n",
        "\n",
        "# Test different batch sizes\n",
        "batch_sizes = [1, 5, 10, 25, 50, 100]\n",
        "results = []\n",
        "\n",
        "print(\"Batch Size Impact Analysis\")\n",
        "print(\"Feature Service: underwriting_v1\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    entity_values = generate_customer_ids(batch_size)\n",
        "    latencies = quick_latency_test(\n",
        "        feature_service=\"underwriting_v1\",\n",
        "        entity_key=\"customer_id\",\n",
        "        entity_values=entity_values,\n",
        "        num_requests=20\n",
        "    )\n",
        "    if latencies:\n",
        "        sorted_l = sorted(latencies)\n",
        "        results.append({\n",
        "            'batch_size': batch_size,\n",
        "            'mean': statistics.mean(latencies),\n",
        "            'p50': sorted_l[len(sorted_l)//2],\n",
        "            'p99': sorted_l[int(len(sorted_l)*0.99)],\n",
        "        })\n",
        "    print()\n",
        "\n",
        "# Display results\n",
        "results_df = pd.DataFrame(results)\n",
        "display(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "if results_df is not None and len(results_df) > 0:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    ax.plot(results_df['batch_size'], results_df['mean'], 'b-o', label='Mean')\n",
        "    ax.plot(results_df['batch_size'], results_df['p50'], 'g--o', label='P50')\n",
        "    ax.plot(results_df['batch_size'], results_df['p99'], 'r-o', label='P99')\n",
        "    \n",
        "    # Add target line\n",
        "    ax.axhline(y=50, color='orange', linestyle='--', label='Target P99 (50ms)')\n",
        "    \n",
        "    ax.set_xlabel('Batch Size')\n",
        "    ax.set_ylabel('Latency (ms)')\n",
        "    ax.set_title('Latency vs Batch Size - Underwriting Feature Service')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No results to visualize. Ensure the feature server is running.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Recommendations\n",
        "\n",
        "Based on the benchmark results, consider the following optimizations:\n",
        "\n",
        "1. **For Real-Time PCM**: Keep batch size â‰¤ 10 for sub-50ms latency\n",
        "2. **For Batch Claims**: Larger batch sizes (50-100) are acceptable\n",
        "3. **For Streaming DSS**: Single entity lookups recommended for lowest latency\n",
        "\n",
        "### Scaling Recommendations\n",
        "\n",
        "- **DynamoDB**: Enable auto-scaling and consider DAX for caching\n",
        "- **Feature Server**: Deploy behind load balancer with multiple instances\n",
        "- **Redshift**: Use appropriate cluster size for offline materialization"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
